# 1. 什么是AR流（自回归流，Autoregressive Flow）？

**自回归**原理是将高维变量的联合概率，写成有序条件概率的连乘公式：

$$
p(x_1, \ldots, x_D) = p(x_1) p(x_2|x_1) p(x_3|x_1,x_2)\ldots p(x_D|x_1,\ldots,x_{D-1})

$$

**自回归流（Autoregressive Flow, AR Flow）**利用这种结构，将每一维的变换只依赖前面的变量，具体表达为：

- 第$i$维输出仅依赖于前面$1\sim i$维的输入
- 可以写成如下变换：

$$
\begin{aligned}
y_1 &= f_1(x_1) \\
y_2 &= f_2(x_2; x_1) \\
y_3 &= f_3(x_3; x_1, x_2) \\
\vdots \\
y_D &= f_D(x_D; x_1, \ldots, x_{D-1})
\end{aligned}

$$

其中$f_i$通常可参数化为神经网络函数。

---

## 2. 常见自回归流结构

**MAF（Masked Autoregressive Flow）、IAF（Inverse Autoregressive Flow）、MADE**等，都是自回归流的代表。

举例，**MAF**的每一步变换为：

$$
y_i = \mu_i(x_{1:i-1}) + \sigma_i(x_{1:i-1}) \cdot x_i

$$

其中$\mu_i,\sigma_i$均由前$i-1$个变量经过神经网络得到。

---

## 3. 雅可比矩阵结构

对自回归变换，Jacobian矩阵是**下三角矩阵**，因为每一维$y_i$只依赖于$1\sim i$的$x$：

$$
J_{ij} = \frac{\partial y_i}{\partial x_j} = 
\begin{cases}
* \quad \text{if } j\le i \\
0 \quad \text{if } j > i
\end{cases}

$$

**行列式计算依然高效**（等于对角线元素乘积），因为三角矩阵的行列式就是主对角线元素的连乘。

---

## 4. AR流vs. 耦合层（Coupling Layer）


| 特点         | AR流（自回归流）                                     | 耦合层（Coupling Layer）                 |
| -------------- | ------------------------------------------------------ | ------------------------------------------ |
| 变量依赖方式 | 每一维依赖前面所有变量（强依赖/强建模能力）          | 只依赖分组中的一部分（弱依赖，但可并行） |
| 并行计算     | **训练可并行，生成时（采样）需严格按顺序、不能并行** | 训练/生成都可并行（一半维度按组变换）    |
| 雅可比结构   | 下三角矩阵，行列式好算                               | 对角阵或块对角阵，行列式好算             |
| 典型模型     | MAF, MADE, PixelCNN, PixelRNN等                      | RealNVP, Glow等                          |
| 应用         | 常用于时间序列、序列建模、图像逐像素建模             | 适用通用流式建模，适合高纬度生成         |

---

## 5. 优缺点

**AR流优点：**

- 表达能力强，可以较容易建模复杂、高阶的变量相关性（所有变量之间都可能被捕捉到非线性耦合）

**AR流缺点：**

- **生成速度慢**，必须“一步步顺序采样”，不能并行
- 训练时可以并行（因为条件概率都现成）

**耦合层优点：**

- 并行采样，速度快，结构更简洁
- 适合高维数据建模

**耦合层缺点：**

- 表达能力相比自回归稍弱，必须堆叠更多层或多种交错分组

---

## 6. 场景举例

- 想建模**像素序列**（如PixelRNN、PixelCNN）或者时间序列 → 用AR流
- 想做高效大数据量采样/生成，比如高清图像流式建模 → 用耦合层为主的Flow模型

---

## 7. 总结一句话

> **AR流（自回归流）是流模型中每一维依次条件建模的方式，表达力强但并行性略逊，和耦合层一起构成了流式生成建模的两大主流设计。**
