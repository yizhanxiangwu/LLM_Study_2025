**生成模型**是指可以学习训练数据分布，并能够生成“看起来像”训练数据的新样本的模型。  
这是机器学习，特别是无监督学习、深度学习中的一个重要分支。

### 生成模型常见方法分为以下几大类：

---

#### 1. 基于概率密度建模的方法
这些方法直接对数据的概率分布进行参数化和建模。

- **高斯混合模型（GMM）：** 用多个高斯分布组合来拟合复杂分布。
- **自回归模型（Autoregressive Models）：** 如 PixelRNN、PixelCNN，每个像素/单元依赖前面的值，序列生成精准。
- **归一化流（Normalizing Flows）：** 通过可逆变换学习复杂分布，支持精确采样与概率计算。
- **变分自编码器（VAE，Variational Autoencoder）：** 利用神经网络与概率推断结合的生成模型，将数据编码为潜变量分布，再样本化重构。

---

#### 2. 显式模型 VS 隐式模型
- **显式模型（Explicit Models）**  
  直接建模概率分布（如 VAE、Normalizing Flows）。
- **隐式模型（Implicit Models）**  
  不直接建模概率分布，而通过某种机制让生成样本看起来像真实数据。例如 GAN。

---

#### 3. 对抗生成方法
- **生成对抗网络（GAN, Generative Adversarial Network）：**  
  由生成器（Generator）和判别器（Discriminator）相互博弈，一方学生成真数据，另一方学分辨真假数据。

---

#### 4. 扩散模型与能量模型
- **扩散模型（Diffusion Models）**  
  通过向数据中逐步加噪声，然后学习逆过程还原数据，最近在图像、音频生成方面发展非常快。
- **能量基模型（EBM, Energy-Based Models）**  
  设定一个能量函数，低能量对应高概率区间，不直接建模分布，而基于能量面采样和优化。

---

#### 5. Flow Matching、分数模型等新兴方法  
- **Flow Matching、Score-based Model**  
  侧重学习从简单分布到复杂分布间的动力学或随机过程优化路径，是对流模型和扩散模型的泛化。

---

### 总结一张表：

| 方法类别      | 代表方法                   | 原理简述                             | 优点            |
| ----------- | --------------------- | --------------------------------- | -------------- |
| 显式概率模型   | GMM、VAE、NF           | 直接建模概率密度/采样                 | 严格概率建模 |
| 隐式概率模型   | GAN                   | 判别机制驱动分布收敛                   | 采样高质量      |
| 扩散/流式方法  | 扩散模型、Flow Matching | 基于随机过程或常微分方程学习分布转移路径 | 稳定、收敛性好   |
| 自回归       | PixelCNN/PixelRNN      | 分布分解为局部有序条件概率               | 精确建模序列     |
| 能量基模型    | EBM                    | 定义能量函数，高数值对应低概率           | 泛化能力强       |

---

### 结论

生成模型无论是哪种分支，本质都是**通过神经网络等方法，学会真实数据的潜在分布，从而能“造出”新样本**。不同方法各有优劣，具体选型取决于任务需求、数据类型和计算资源。
